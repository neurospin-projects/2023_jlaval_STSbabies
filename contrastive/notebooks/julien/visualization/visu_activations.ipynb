{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import torch\n",
    "import omegaconf\n",
    "import glob\n",
    "import hydra\n",
    "from omegaconf import DictConfig\n",
    "from omegaconf import OmegaConf\n",
    "from contrastive.utils.config import process_config\n",
    "from contrastive.data.datamodule import DataModule_Evaluation\n",
    "from contrastive.models.contrastive_learner_fusion import \\\n",
    "    ContrastiveLearnerFusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = '/volatile/jl277509/Runs/02_STS_babies/Program/Output/ORBITAL_12-layer_k7/16-19-26_238'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:contrastive.utils.config: Working directory : /volatile/jl277509/Runs/02_STS_babies/Program/2023_jlaval_STSbabies/contrastive/notebooks/julien/visualization\n",
      "INFO:utils.py: Train/val size partitions = [18946, 2105]\n",
      "INFO:utils.py: Seed for train/val split is 1\n",
      "INFO:utils.py: test set size: (0, 30, 38, 22, 1)\n",
      "INFO:utils.py: Length of train dataframe = 18946\n",
      "INFO:utils.py: Length of val dataframe = 2105\n",
      "INFO:utils.py: Length of train_val dataframe = 21051\n",
      "INFO:create_datasets.py: foldlabel data NOT requested. Foldlabel data NOT loaded\n",
      "INFO:create_datasets.py: distbottom data NOT requested. Distbottom data NOT loaded\n",
      "INFO:contrastive_learner_fusion.py: n_datasets 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No trained_model.pt saved. Create a new instance and load weights.\n"
     ]
    }
   ],
   "source": [
    "config_path = os.path.join(model_path, '.hydra/config.yaml')\n",
    "config = omegaconf.OmegaConf.load(config_path)\n",
    "config = process_config(config)\n",
    "\n",
    "config.apply_augmentations = False\n",
    "config.with_labels = False\n",
    "\n",
    "# create new models in mode visualisation\n",
    "data_module = DataModule_Evaluation(config)\n",
    "data_module.setup(stage='validate')\n",
    "\n",
    "# create a new instance of the current model version,\n",
    "# then load hydra weights.\n",
    "print(\"No trained_model.pt saved. Create a new instance and load weights.\")\n",
    "\n",
    "model = ContrastiveLearnerFusion(config, sample_data=data_module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ContrastiveLearnerFusion(\n",
       "  (backbones): ModuleList(\n",
       "    (0): ConvNet(\n",
       "      (encoder): Sequential(\n",
       "        (conv0): Conv3d(1, 32, kernel_size=(7, 7, 7), stride=(1, 1, 1), padding=(3, 3, 3))\n",
       "        (norm0): BatchNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (LeakyReLU0): LeakyReLU(negative_slope=0.01)\n",
       "        (DropOut0): Dropout3d(p=0.05, inplace=False)\n",
       "        (conv0a): Conv3d(32, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
       "        (norm0a): BatchNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (LeakyReLU0a): LeakyReLU(negative_slope=0.01)\n",
       "        (DropOut0a): Dropout3d(p=0.05, inplace=False)\n",
       "        (conv0b): Conv3d(32, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
       "        (norm0b): BatchNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (LeakyReLU0b): LeakyReLU(negative_slope=0.01)\n",
       "        (DropOut0b): Dropout3d(p=0.05, inplace=False)\n",
       "        (conv0c): Conv3d(32, 32, kernel_size=(4, 4, 4), stride=(2, 2, 2), padding=(1, 1, 1))\n",
       "        (norm0c): BatchNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (LeakyReLU0c): LeakyReLU(negative_slope=0.01)\n",
       "        (DropOut0c): Dropout3d(p=0.05, inplace=False)\n",
       "        (conv1): Conv3d(32, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
       "        (norm1): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (LeakyReLU1): LeakyReLU(negative_slope=0.01)\n",
       "        (DropOut1): Dropout3d(p=0.05, inplace=False)\n",
       "        (conv1a): Conv3d(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
       "        (norm1a): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (LeakyReLU1a): LeakyReLU(negative_slope=0.01)\n",
       "        (DropOut1a): Dropout3d(p=0.05, inplace=False)\n",
       "        (conv1b): Conv3d(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
       "        (norm1b): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (LeakyReLU1b): LeakyReLU(negative_slope=0.01)\n",
       "        (DropOut1b): Dropout3d(p=0.05, inplace=False)\n",
       "        (conv1c): Conv3d(64, 64, kernel_size=(4, 4, 4), stride=(2, 2, 2), padding=(1, 1, 1))\n",
       "        (norm1c): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (LeakyReLU1c): LeakyReLU(negative_slope=0.01)\n",
       "        (DropOut1c): Dropout3d(p=0.05, inplace=False)\n",
       "        (conv2): Conv3d(64, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
       "        (norm2): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (LeakyReLU2): LeakyReLU(negative_slope=0.01)\n",
       "        (DropOut2): Dropout3d(p=0.05, inplace=False)\n",
       "        (conv2a): Conv3d(128, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
       "        (norm2a): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (LeakyReLU2a): LeakyReLU(negative_slope=0.01)\n",
       "        (DropOut2a): Dropout3d(p=0.05, inplace=False)\n",
       "        (conv2b): Conv3d(128, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
       "        (norm2b): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (LeakyReLU2b): LeakyReLU(negative_slope=0.01)\n",
       "        (DropOut2b): Dropout3d(p=0.05, inplace=False)\n",
       "        (conv2c): Conv3d(128, 128, kernel_size=(4, 4, 4), stride=(2, 2, 2), padding=(1, 1, 1))\n",
       "        (norm2c): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (LeakyReLU2c): LeakyReLU(negative_slope=0.01)\n",
       "        (DropOut2c): Dropout3d(p=0.05, inplace=False)\n",
       "        (Flatten): Flatten(start_dim=1, end_dim=-1)\n",
       "        (Linear): Linear(in_features=3072, out_features=256, bias=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (converter): Sequential()\n",
       "  (projection_head): ProjectionHead(\n",
       "    (layers): Sequential(\n",
       "      (Linear0): Linear(in_features=256, out_features=256, bias=True)\n",
       "      (LeakyReLU0): LeakyReLU(negative_slope=0.01)\n",
       "      (DropOut0): Dropout(p=0, inplace=False)\n",
       "      (Linear1): Linear(in_features=256, out_features=256, bias=True)\n",
       "      (LeakyReLU1): LeakyReLU(negative_slope=0.01)\n",
       "      (DropOut1): Dropout(p=0, inplace=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paths = model_path+\"/logs/*/version_0/checkpoints\"+r'/*.ckpt'\n",
    "files = glob.glob(paths)\n",
    "ckpt_path = files[0]\n",
    "checkpoint = torch.load(\n",
    "            ckpt_path, map_location=torch.device(config.device))\n",
    "model.load_state_dict(checkpoint['state_dict'])\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model's state_dict:\n",
      "backbones.0.encoder.conv0.weight \t torch.Size([32, 1, 7, 7, 7])\n",
      "backbones.0.encoder.conv0.bias \t torch.Size([32])\n",
      "backbones.0.encoder.norm0.weight \t torch.Size([32])\n",
      "backbones.0.encoder.norm0.bias \t torch.Size([32])\n",
      "backbones.0.encoder.norm0.running_mean \t torch.Size([32])\n",
      "backbones.0.encoder.norm0.running_var \t torch.Size([32])\n",
      "backbones.0.encoder.norm0.num_batches_tracked \t torch.Size([])\n",
      "backbones.0.encoder.conv0a.weight \t torch.Size([32, 32, 3, 3, 3])\n",
      "backbones.0.encoder.conv0a.bias \t torch.Size([32])\n",
      "backbones.0.encoder.norm0a.weight \t torch.Size([32])\n",
      "backbones.0.encoder.norm0a.bias \t torch.Size([32])\n",
      "backbones.0.encoder.norm0a.running_mean \t torch.Size([32])\n",
      "backbones.0.encoder.norm0a.running_var \t torch.Size([32])\n",
      "backbones.0.encoder.norm0a.num_batches_tracked \t torch.Size([])\n",
      "backbones.0.encoder.conv0b.weight \t torch.Size([32, 32, 3, 3, 3])\n",
      "backbones.0.encoder.conv0b.bias \t torch.Size([32])\n",
      "backbones.0.encoder.norm0b.weight \t torch.Size([32])\n",
      "backbones.0.encoder.norm0b.bias \t torch.Size([32])\n",
      "backbones.0.encoder.norm0b.running_mean \t torch.Size([32])\n",
      "backbones.0.encoder.norm0b.running_var \t torch.Size([32])\n",
      "backbones.0.encoder.norm0b.num_batches_tracked \t torch.Size([])\n",
      "backbones.0.encoder.conv0c.weight \t torch.Size([32, 32, 4, 4, 4])\n",
      "backbones.0.encoder.conv0c.bias \t torch.Size([32])\n",
      "backbones.0.encoder.norm0c.weight \t torch.Size([32])\n",
      "backbones.0.encoder.norm0c.bias \t torch.Size([32])\n",
      "backbones.0.encoder.norm0c.running_mean \t torch.Size([32])\n",
      "backbones.0.encoder.norm0c.running_var \t torch.Size([32])\n",
      "backbones.0.encoder.norm0c.num_batches_tracked \t torch.Size([])\n",
      "backbones.0.encoder.conv1.weight \t torch.Size([64, 32, 3, 3, 3])\n",
      "backbones.0.encoder.conv1.bias \t torch.Size([64])\n",
      "backbones.0.encoder.norm1.weight \t torch.Size([64])\n",
      "backbones.0.encoder.norm1.bias \t torch.Size([64])\n",
      "backbones.0.encoder.norm1.running_mean \t torch.Size([64])\n",
      "backbones.0.encoder.norm1.running_var \t torch.Size([64])\n",
      "backbones.0.encoder.norm1.num_batches_tracked \t torch.Size([])\n",
      "backbones.0.encoder.conv1a.weight \t torch.Size([64, 64, 3, 3, 3])\n",
      "backbones.0.encoder.conv1a.bias \t torch.Size([64])\n",
      "backbones.0.encoder.norm1a.weight \t torch.Size([64])\n",
      "backbones.0.encoder.norm1a.bias \t torch.Size([64])\n",
      "backbones.0.encoder.norm1a.running_mean \t torch.Size([64])\n",
      "backbones.0.encoder.norm1a.running_var \t torch.Size([64])\n",
      "backbones.0.encoder.norm1a.num_batches_tracked \t torch.Size([])\n",
      "backbones.0.encoder.conv1b.weight \t torch.Size([64, 64, 3, 3, 3])\n",
      "backbones.0.encoder.conv1b.bias \t torch.Size([64])\n",
      "backbones.0.encoder.norm1b.weight \t torch.Size([64])\n",
      "backbones.0.encoder.norm1b.bias \t torch.Size([64])\n",
      "backbones.0.encoder.norm1b.running_mean \t torch.Size([64])\n",
      "backbones.0.encoder.norm1b.running_var \t torch.Size([64])\n",
      "backbones.0.encoder.norm1b.num_batches_tracked \t torch.Size([])\n",
      "backbones.0.encoder.conv1c.weight \t torch.Size([64, 64, 4, 4, 4])\n",
      "backbones.0.encoder.conv1c.bias \t torch.Size([64])\n",
      "backbones.0.encoder.norm1c.weight \t torch.Size([64])\n",
      "backbones.0.encoder.norm1c.bias \t torch.Size([64])\n",
      "backbones.0.encoder.norm1c.running_mean \t torch.Size([64])\n",
      "backbones.0.encoder.norm1c.running_var \t torch.Size([64])\n",
      "backbones.0.encoder.norm1c.num_batches_tracked \t torch.Size([])\n",
      "backbones.0.encoder.conv2.weight \t torch.Size([128, 64, 3, 3, 3])\n",
      "backbones.0.encoder.conv2.bias \t torch.Size([128])\n",
      "backbones.0.encoder.norm2.weight \t torch.Size([128])\n",
      "backbones.0.encoder.norm2.bias \t torch.Size([128])\n",
      "backbones.0.encoder.norm2.running_mean \t torch.Size([128])\n",
      "backbones.0.encoder.norm2.running_var \t torch.Size([128])\n",
      "backbones.0.encoder.norm2.num_batches_tracked \t torch.Size([])\n",
      "backbones.0.encoder.conv2a.weight \t torch.Size([128, 128, 3, 3, 3])\n",
      "backbones.0.encoder.conv2a.bias \t torch.Size([128])\n",
      "backbones.0.encoder.norm2a.weight \t torch.Size([128])\n",
      "backbones.0.encoder.norm2a.bias \t torch.Size([128])\n",
      "backbones.0.encoder.norm2a.running_mean \t torch.Size([128])\n",
      "backbones.0.encoder.norm2a.running_var \t torch.Size([128])\n",
      "backbones.0.encoder.norm2a.num_batches_tracked \t torch.Size([])\n",
      "backbones.0.encoder.conv2b.weight \t torch.Size([128, 128, 3, 3, 3])\n",
      "backbones.0.encoder.conv2b.bias \t torch.Size([128])\n",
      "backbones.0.encoder.norm2b.weight \t torch.Size([128])\n",
      "backbones.0.encoder.norm2b.bias \t torch.Size([128])\n",
      "backbones.0.encoder.norm2b.running_mean \t torch.Size([128])\n",
      "backbones.0.encoder.norm2b.running_var \t torch.Size([128])\n",
      "backbones.0.encoder.norm2b.num_batches_tracked \t torch.Size([])\n",
      "backbones.0.encoder.conv2c.weight \t torch.Size([128, 128, 4, 4, 4])\n",
      "backbones.0.encoder.conv2c.bias \t torch.Size([128])\n",
      "backbones.0.encoder.norm2c.weight \t torch.Size([128])\n",
      "backbones.0.encoder.norm2c.bias \t torch.Size([128])\n",
      "backbones.0.encoder.norm2c.running_mean \t torch.Size([128])\n",
      "backbones.0.encoder.norm2c.running_var \t torch.Size([128])\n",
      "backbones.0.encoder.norm2c.num_batches_tracked \t torch.Size([])\n",
      "backbones.0.encoder.Linear.weight \t torch.Size([256, 3072])\n",
      "backbones.0.encoder.Linear.bias \t torch.Size([256])\n",
      "projection_head.layers.Linear0.weight \t torch.Size([256, 256])\n",
      "projection_head.layers.Linear0.bias \t torch.Size([256])\n",
      "projection_head.layers.Linear1.weight \t torch.Size([256, 256])\n",
      "projection_head.layers.Linear1.bias \t torch.Size([256])\n"
     ]
    }
   ],
   "source": [
    "print(\"Model's state_dict:\")\n",
    "for param_tensor in model.state_dict():\n",
    "    print(param_tensor, \"\\t\", model.state_dict()[param_tensor].size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visu kernels #is it the right axis ??\n",
    "kernels = model.state_dict()['backbones.0.encoder.conv0.weight'].detach()\n",
    "for i in range(32):\n",
    "    for k in range(7):\n",
    "        kernel = kernels[i,0,k,:,:]\n",
    "        plt.imshow(kernel)\n",
    "        plt.savefig(f'/home/jl277509/Documents/plot_kernels/kernel_{i}_slice{k}.png')\n",
    "        plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load image\n",
    "skeletons = np.load('/neurospin/dico/data/deep_folding/current/datasets/hcp/crops/2mm/ORBITAL/mask/Lskeleton.npy')\n",
    "skeletons = skeletons != 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find a good subject, with an interruption, visible in a plane ?\n",
    "# first save the slices of this subject ? Or add to the subplot ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 1, 30, 38, 22])\n"
     ]
    }
   ],
   "source": [
    "data = skeletons[0]\n",
    "data = np.transpose(data, [3, 0, 1, 2])\n",
    "data[data > 0] = 1\n",
    "data = data.astype(np.float32)\n",
    "data = torch.from_numpy(data)\n",
    "data.unsqueeze_(0)\n",
    "data.unsqueeze_(0)\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "# which place contains orbital ??\n",
    "#for k in range(0, 22):\n",
    "#    plt.imshow(data[:,:,k,0])\n",
    "#    plt.savefig(f'/home/jl277509/Documents/orbital_slices/slice_{k}.png')\n",
    "#    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.conv1 doesn't exist !!\n",
    "# model.backbones[0].encoder.conv0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv = 'conv1'\n",
    "# NB: also need to change conv1 below ...\n",
    "step_data = 2 # because of stride # conv0: 1, conv1: 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize feature maps\n",
    "activation = {}\n",
    "def get_activation(name):\n",
    "    def hook(model, input, output):\n",
    "        activation[name] = output.detach()\n",
    "    return hook\n",
    "\n",
    "model.backbones[0].encoder.conv1.register_forward_hook(get_activation(conv))\n",
    "output = model(data)\n",
    "\n",
    "act = activation[conv].squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "nrows=4\n",
    "for k in range(0, act.shape[-1]):\n",
    "    fig, axs = plt.subplots(figsize=(((act.shape[0])//nrows)*4, 4*4), ncols=(act.shape[0])//nrows, nrows=nrows+1)\n",
    "    for idx in range(act.shape[0]):\n",
    "        axs[(idx)%nrows, (idx)//nrows].imshow(act[idx, :,:,k])\n",
    "    axs[(act.shape[0]-1)%nrows+1, (act.shape[0]-1)//nrows].imshow(data[0,0,0,:,:,k*step_data])\n",
    "    save_dir = f'/home/jl277509/Documents/plot_conv/{conv}/'\n",
    "    if not os.path.isdir(save_dir):\n",
    "        os.mkdir(save_dir)\n",
    "    plt.savefig(os.path.join(save_dir, f'{conv}_slice_{k}.png'))\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize conv filter # need to adapt to 3D\n",
    "kernels = model.backbones[0].encoder.conv0.weight.detach()\n",
    "fig, axarr = plt.subplots(kernels.size(0))\n",
    "for idx in range(kernels.size(0)):\n",
    "    axarr[idx].imshow(kernels[idx].squeeze())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
